{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Understanding and Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21417"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake)\n",
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "21412  'Fully committed' NATO backs new U.S. approach...   \n",
       "21413  LexisNexis withdrew two products from Chinese ...   \n",
       "21414  Minsk cultural hub becomes haven from authorities   \n",
       "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
       "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date  label  \n",
       "0      December 31, 2017      1  \n",
       "1      December 31, 2017      1  \n",
       "2      December 30, 2017      1  \n",
       "3      December 29, 2017      1  \n",
       "4      December 25, 2017      1  \n",
       "...                  ...    ...  \n",
       "21412   August 22, 2017       0  \n",
       "21413   August 22, 2017       0  \n",
       "21414   August 22, 2017       0  \n",
       "21415   August 22, 2017       0  \n",
       "21416   August 22, 2017       0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake['label'] = 1\n",
    "true['label']=0\n",
    "\n",
    "df_concated = pd.concat([fake, true])\n",
    "permutation = np.random.permutation(len(df_concated))\n",
    "df = df_concated.iloc[permutation]\n",
    "df_concated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44689, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count_per_column = df.isnull().sum()\n",
    "\n",
    "print(nan_count_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>More than 30 rescued in Afghan raid on Taliban...</td>\n",
       "      <td>LASHKAR GAH, Afghanistan (Reuters) - Afghan an...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20156</th>\n",
       "      <td>Kurds will find it hard to implement independe...</td>\n",
       "      <td>CAIRO (Reuters) - Iraqi Kurdish leaders must b...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23278</th>\n",
       "      <td>POLICE STATE END-RUN: DHS Wants Control of U.S...</td>\n",
       "      <td>21st Century Wire says Here s a classic case o...</td>\n",
       "      <td>Middle-east</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>House GOP To Waste MORE Time And Money With S...</td>\n",
       "      <td>When House Democrats staged a 26-hour sit-in t...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21098</th>\n",
       "      <td>IF YOU’RE EASILY OFFENDED, Don’t Watch This Am...</td>\n",
       "      <td>You re gonna love this patriot! He speaks for ...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15885</th>\n",
       "      <td>FLASHBACK: KING OBAMA COMMUTES SENTENCES OF 22...</td>\n",
       "      <td>Just making room for Hillary President Obama t...</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18988</th>\n",
       "      <td>No Ex-President In 100 Yrs Has Set Up A Shadow...</td>\n",
       "      <td>FOX News  Catherine Herridge exposes the dirty...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>Jimmy John’s Thinks $1 Sandwiches Are Going T...</td>\n",
       "      <td>This week Jimmy John s sandwiches offered $1 s...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>U.S. attorney general did not order FBI to wit...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. Attorney General L...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>Blumenthal: It ‘Could Be Perjury’ If Sessions...</td>\n",
       "      <td>Sen. Richard Blumenthal was deeply disturbed b...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "14444  More than 30 rescued in Afghan raid on Taliban...   \n",
       "20156  Kurds will find it hard to implement independe...   \n",
       "23278  POLICE STATE END-RUN: DHS Wants Control of U.S...   \n",
       "5600    House GOP To Waste MORE Time And Money With S...   \n",
       "21098  IF YOU’RE EASILY OFFENDED, Don’t Watch This Am...   \n",
       "...                                                  ...   \n",
       "15885  FLASHBACK: KING OBAMA COMMUTES SENTENCES OF 22...   \n",
       "18988  No Ex-President In 100 Yrs Has Set Up A Shadow...   \n",
       "6748    Jimmy John’s Thinks $1 Sandwiches Are Going T...   \n",
       "7625   U.S. attorney general did not order FBI to wit...   \n",
       "1213    Blumenthal: It ‘Could Be Perjury’ If Sessions...   \n",
       "\n",
       "                                                    text       subject  label  \n",
       "14444  LASHKAR GAH, Afghanistan (Reuters) - Afghan an...     worldnews      0  \n",
       "20156  CAIRO (Reuters) - Iraqi Kurdish leaders must b...     worldnews      0  \n",
       "23278  21st Century Wire says Here s a classic case o...   Middle-east      1  \n",
       "5600   When House Democrats staged a 26-hour sit-in t...          News      1  \n",
       "21098  You re gonna love this patriot! He speaks for ...     left-news      1  \n",
       "...                                                  ...           ...    ...  \n",
       "15885  Just making room for Hillary President Obama t...      politics      1  \n",
       "18988  FOX News  Catherine Herridge exposes the dirty...     left-news      1  \n",
       "6748   This week Jimmy John s sandwiches offered $1 s...          News      1  \n",
       "7625   WASHINGTON (Reuters) - U.S. Attorney General L...  politicsNews      0  \n",
       "1213   Sen. Richard Blumenthal was deeply disturbed b...          News      1  \n",
       "\n",
       "[44689 rows x 4 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_date = df.drop(columns=['date'])\n",
    "\n",
    "df_no_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to make text Preprocessing, where are going through these steps: \n",
    "* Lowercasing\n",
    "* Removing Special Characters and Punctuation\n",
    "* Tokenization\n",
    "* Removing Stopwords\n",
    "* Lemmatization (We'll prefer this over stemming as it's generally more effective for understanding the context of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')  # for stopwords\n",
    "nltk.download('wordnet')  # for lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_subject(subject):\n",
    "    # Convert text to lowercase\n",
    "    subject = subject.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    subject = ''.join(char for char in subject if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(subject)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_title(title):\n",
    "    # Convert text to lowercase\n",
    "    title = title.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    title = ''.join(char for char in title if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(title)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it contains a column named 'text'\n",
    "df_no_date['cleaned_text'] = df_no_date['text'].apply(preprocess_text)\n",
    "df_no_date['cleaned_subject'] = df_no_date['subject'].apply(preprocess_subject)\n",
    "df_no_date['cleaned_title'] = df_no_date['title'].apply(preprocess_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_date.drop(columns=['title', 'subject', 'text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_subject</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14444</th>\n",
       "      <td>0</td>\n",
       "      <td>lashkar gah afghanistan reuters afghan foreign...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>30 rescued afghan raid taliban prison</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20156</th>\n",
       "      <td>0</td>\n",
       "      <td>cairo reuters iraqi kurdish leader must prepar...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>kurd find hard implement independence say iraq...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23278</th>\n",
       "      <td>1</td>\n",
       "      <td>21st century wire say classic case problem rea...</td>\n",
       "      <td>middleeast</td>\n",
       "      <td>police state endrun dhs want control u election</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5600</th>\n",
       "      <td>1</td>\n",
       "      <td>house democrat staged 26hour sitin protest hou...</td>\n",
       "      <td>news</td>\n",
       "      <td>house gop waste time money silly investigation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21098</th>\n",
       "      <td>1</td>\n",
       "      <td>gon na love patriot speaks every one u 100 fed...</td>\n",
       "      <td>leftnews</td>\n",
       "      <td>youre easily offended dont watch american past...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15885</th>\n",
       "      <td>1</td>\n",
       "      <td>making room hillary president obama today anno...</td>\n",
       "      <td>politics</td>\n",
       "      <td>flashback king obama commute sentence 22 drug ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18988</th>\n",
       "      <td>1</td>\n",
       "      <td>fox news catherine herridge expose dirty tacti...</td>\n",
       "      <td>leftnews</td>\n",
       "      <td>expresident 100 yr set shadow governmentgone l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6748</th>\n",
       "      <td>1</td>\n",
       "      <td>week jimmy john sandwich offered 1 sub promoti...</td>\n",
       "      <td>news</td>\n",
       "      <td>jimmy john think 1 sandwich going make u forgi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7625</th>\n",
       "      <td>0</td>\n",
       "      <td>washington reuters u attorney general loretta ...</td>\n",
       "      <td>politicsnews</td>\n",
       "      <td>u attorney general order fbi withhold clintonr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1213</th>\n",
       "      <td>1</td>\n",
       "      <td>sen richard blumenthal deeply disturbed james ...</td>\n",
       "      <td>news</td>\n",
       "      <td>blumenthal could perjury session hid third mee...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                       cleaned_text  \\\n",
       "14444      0  lashkar gah afghanistan reuters afghan foreign...   \n",
       "20156      0  cairo reuters iraqi kurdish leader must prepar...   \n",
       "23278      1  21st century wire say classic case problem rea...   \n",
       "5600       1  house democrat staged 26hour sitin protest hou...   \n",
       "21098      1  gon na love patriot speaks every one u 100 fed...   \n",
       "...      ...                                                ...   \n",
       "15885      1  making room hillary president obama today anno...   \n",
       "18988      1  fox news catherine herridge expose dirty tacti...   \n",
       "6748       1  week jimmy john sandwich offered 1 sub promoti...   \n",
       "7625       0  washington reuters u attorney general loretta ...   \n",
       "1213       1  sen richard blumenthal deeply disturbed james ...   \n",
       "\n",
       "      cleaned_subject                                      cleaned_title  \n",
       "14444       worldnews              30 rescued afghan raid taliban prison  \n",
       "20156       worldnews  kurd find hard implement independence say iraq...  \n",
       "23278      middleeast    police state endrun dhs want control u election  \n",
       "5600             news  house gop waste time money silly investigation...  \n",
       "21098        leftnews  youre easily offended dont watch american past...  \n",
       "...               ...                                                ...  \n",
       "15885        politics  flashback king obama commute sentence 22 drug ...  \n",
       "18988        leftnews  expresident 100 yr set shadow governmentgone l...  \n",
       "6748             news  jimmy john think 1 sandwich going make u forgi...  \n",
       "7625     politicsnews  u attorney general order fbi withhold clintonr...  \n",
       "1213             news  blumenthal could perjury session hid third mee...  \n",
       "\n",
       "[44689 rows x 4 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_date.to_csv('Preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "label                0\n",
      "cleaned_text       632\n",
      "cleaned_subject      0\n",
      "cleaned_title        0\n",
      "dtype: int64\n",
      "Total missing values in the dataset: 632\n"
     ]
    }
   ],
   "source": [
    "# Loading DataFrame to check for missing values\n",
    "df = pd.read_csv('Preprocessed.csv')\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values in each column\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# Calculate the total number of missing values in the DataFrame\n",
    "total_missing_values = missing_values_count.sum()\n",
    "print(\"Total missing values in the dataset:\", total_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our models and other features are able to be included and understood, we remove the rows with missing values below\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (44689, 4)\n",
      "New DataFrame shape after removing rows with NaN: (44057, 4)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values in cleaned_text as seen from the printout above\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new file if needed\n",
    "df_cleaned.to_csv('Preprocessed_clean.csv', index=False)\n",
    "\n",
    "# Optionally, print the shape to see how many rows were dropped\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"New DataFrame shape after removing rows with NaN:\", df_cleaned.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we vectorize the dataset to enable counting tokens in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                       cleaned_text cleaned_subject  \\\n",
      "0      0  lashkar gah afghanistan reuters afghan foreign...       worldnews   \n",
      "1      0  cairo reuters iraqi kurdish leader must prepar...       worldnews   \n",
      "2      1  21st century wire say classic case problem rea...      middleeast   \n",
      "3      1  house democrat staged 26hour sitin protest hou...            news   \n",
      "4      1  gon na love patriot speaks every one u 100 fed...        leftnews   \n",
      "\n",
      "                                       cleaned_title  token_count  \n",
      "0              30 rescued afghan raid taliban prison           30  \n",
      "1  kurd find hard implement independence say iraq...           72  \n",
      "2    police state endrun dhs want control u election           76  \n",
      "3  house gop waste time money silly investigation...           91  \n",
      "4  youre easily offended dont watch american past...           11  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset\n",
    "subset_df = pd.read_csv('Preprocessed_clean.csv')\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(max_features=512)  # You can adjust this if needed\n",
    "\n",
    "# Apply TF-IDF to the 'cleaned_text' column\n",
    "tfidf_matrix = vectorizer.fit_transform(subset_df['cleaned_text'])\n",
    "\n",
    "# Convert TF-IDF matrix to a DataFrame to manipulate easily\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Count non-zero entries in each row, which represents the number of unique tokens used\n",
    "subset_df['token_count'] = (tfidf_df != 0).sum(axis=1)\n",
    "\n",
    "# Filter out rows where the number of tokens is greater than 512\n",
    "filtered_df = subset_df[subset_df['token_count'] <= 512]\n",
    "\n",
    "# Save the filtered DataFrame\n",
    "filtered_df.to_csv('filtered_dataset.csv', index=False)\n",
    "\n",
    "# Optionally, inspect the first few rows of the filtered DataFrame\n",
    "print(filtered_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        30\n",
      "1        72\n",
      "2        76\n",
      "3        91\n",
      "4        11\n",
      "         ..\n",
      "44052    55\n",
      "44053    10\n",
      "44054    86\n",
      "44055    84\n",
      "44056    46\n",
      "Name: token_count, Length: 44057, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df['token_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the printed results none of the rows have been removed, as none of them are above 512 Below we examine the token_count, to determine wether we are able to use this as a way to create subsets and decrease the size of the dataset We will examine it by looking at both the lowest and highest token_count to ensure we incorporate relevant articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                       cleaned_text  \\\n",
      "33783      1  funny secret travel start believe bleed lyric ...   \n",
      "35062      1  funny secret travel start believe bleed lyric ...   \n",
      "\n",
      "      cleaned_subject                                      cleaned_title  \\\n",
      "33783          usnews  medium tripwire ping pong pizza conspiracy pro...   \n",
      "35062      middleeast  medium tripwire ping pong pizza conspiracy pro...   \n",
      "\n",
      "       token_count  \n",
      "33783          360  \n",
      "35062          360  \n",
      "       label                                       cleaned_text  \\\n",
      "253        1                httpswwwyoutubecomwatchvvkrctn0nevu   \n",
      "792        1                 httpswwwyoutubecomwatchv9lnyxdwzza   \n",
      "936        1  httpsfedupwpenginecomwpcontentuploads201504ent...   \n",
      "1122       1                httpswwwyoutubecomwatchvsh0prtk9sae   \n",
      "2104       1                httpswwwyoutubecomwatchvhxjzbpaf0sk   \n",
      "...      ...                                                ...   \n",
      "41883      1                            httpsyoutubertuxvwjh3a4   \n",
      "43239      1                httpswwwyoutubecomwatchvyedu6dcr9ta   \n",
      "43824      1  httpsfedupwpenginecomwpcontentuploads201504hil...   \n",
      "43856      1                                           divide u   \n",
      "44028      1  chris stevens sean smith glen doherty tyrone wood   \n",
      "\n",
      "       cleaned_subject                                      cleaned_title  \\\n",
      "253           leftnews  tucker carlson out human right exec director p...   \n",
      "792           leftnews  thank babyyes feminist go nut president trump ...   \n",
      "936           politics  httpsfedupwpenginecomwpcontentuploads201504ent...   \n",
      "1122   government news  crybaby nancy pelosi taunt trump healthcare bi...   \n",
      "2104          politics  brilliant rep king call cia director hit job t...   \n",
      "...                ...                                                ...   \n",
      "41883         politics  view whoopi goldberg cohost black people dont ...   \n",
      "43239         politics  youre sexist pig feminist battle tucker carlso...   \n",
      "43824         leftnews  httpsfedupwpenginecomwpcontentuploads201504hil...   \n",
      "43856         leftnews  working class revolt old school jersey patriot...   \n",
      "44028         politics  watch crooked hillary make claim life lost lib...   \n",
      "\n",
      "       token_count  \n",
      "253              0  \n",
      "792              0  \n",
      "936              0  \n",
      "1122             0  \n",
      "2104             0  \n",
      "...            ...  \n",
      "41883            0  \n",
      "43239            0  \n",
      "43824            0  \n",
      "43856            0  \n",
      "44028            0  \n",
      "\n",
      "[134 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the maximum 'token_count'\n",
    "max_token_row = filtered_df[filtered_df['token_count'] == filtered_df['token_count'].max()]\n",
    "\n",
    "# Find the row with the minimum 'token_count'\n",
    "min_token_row = filtered_df[filtered_df['token_count'] == filtered_df['token_count'].min()]\n",
    "\n",
    "# Print the row with the maximum and minimum token count\n",
    "print(max_token_row)\n",
    "print(min_token_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see many rows seem to have 0 tokens, which means these are not understood well by the computer This show a relevance in setting a minimum token_count and creating a subset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: (13834, 5)\n",
      "Class distribution in subset:\n",
      " 1    6917\n",
      "0    6917\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a minimum token count threshold\n",
    "MIN_TOKENS = 70\n",
    "\n",
    "# determine that the DataFrame should include only rows with 'token_count' being greater than or equal to MIN_TOKENS\n",
    "filter_df_tokens = filtered_df[filtered_df['token_count'] >= MIN_TOKENS]\n",
    "\n",
    "# Split the DataFrame into two based on the label\n",
    "tokens_df_class_0 = filter_df_tokens[filter_df_tokens['label'] == 0]\n",
    "tokens_df_class_1 = filter_df_tokens[filter_df_tokens['label'] == 1]\n",
    "\n",
    "# Find the minimum count to balance the dataset\n",
    "min_count = min(len(tokens_df_class_0), len(tokens_df_class_1))\n",
    "\n",
    "# Randomly sample min_count rows from each DataFrame\n",
    "subset_class_0 = tokens_df_class_0.sample(n=min_count, random_state=42)  # Ensures reproducibility\n",
    "subset_class_1 = tokens_df_class_1.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Concatenate the two subsets to form a new balanced DataFrame\n",
    "balanced_subset = pd.concat([subset_class_0, subset_class_1])\n",
    "\n",
    "# Shuffle the rows to ensure random order\n",
    "final_subset = balanced_subset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the subset to a new CSV file\n",
    "final_subset.to_csv('text_dataset.csv', index=False)\n",
    "\n",
    "# Print some information about the subset\n",
    "print(\"Subset shape:\", final_subset.shape)\n",
    "print(\"Class distribution in subset:\\n\", final_subset['label'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e8995cc79525bd2f03bf06fe7351d86fcc477a350b98b661267de2f13aa6250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
