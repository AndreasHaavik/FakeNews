{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets see how the DF looks now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>TTR</th>\n",
       "      <th>text_polarity</th>\n",
       "      <th>text_subjectivity</th>\n",
       "      <th>title_polarity</th>\n",
       "      <th>title_subjectivity</th>\n",
       "      <th>doc_perplexity</th>\n",
       "      <th>1_grams</th>\n",
       "      <th>average_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.844037</td>\n",
       "      <td>0.091481</td>\n",
       "      <td>0.524184</td>\n",
       "      <td>-0.125000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.867938</td>\n",
       "      <td>['donald', 'trump', 'met', 'member', 'nato', '...</td>\n",
       "      <td>6.490826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.683438</td>\n",
       "      <td>0.003001</td>\n",
       "      <td>0.343395</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882860</td>\n",
       "      <td>['washington', 'reuters', 'rick', 'perry', 'pr...</td>\n",
       "      <td>6.385744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.721030</td>\n",
       "      <td>0.073622</td>\n",
       "      <td>0.401345</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883760</td>\n",
       "      <td>['president', 'obama', 'blasted', 'republican'...</td>\n",
       "      <td>6.072961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.667638</td>\n",
       "      <td>0.037264</td>\n",
       "      <td>0.462935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.882882</td>\n",
       "      <td>['male', 'idaho', 'republican', 'daughter', 'c...</td>\n",
       "      <td>6.469388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0.628032</td>\n",
       "      <td>-0.018966</td>\n",
       "      <td>0.479310</td>\n",
       "      <td>-0.800000</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.883120</td>\n",
       "      <td>['kellyanne', 'conway', 'tried', 'spin', 'whit...</td>\n",
       "      <td>6.296496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13829</th>\n",
       "      <td>0</td>\n",
       "      <td>0.659631</td>\n",
       "      <td>-0.030698</td>\n",
       "      <td>0.380595</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.882810</td>\n",
       "      <td>['washington', 'reuters', 'member', 'congress'...</td>\n",
       "      <td>6.514512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13830</th>\n",
       "      <td>1</td>\n",
       "      <td>0.629126</td>\n",
       "      <td>-0.009035</td>\n",
       "      <td>0.312617</td>\n",
       "      <td>-0.066667</td>\n",
       "      <td>0.633333</td>\n",
       "      <td>0.895444</td>\n",
       "      <td>['far', 'video', '530000', 'view', 'make', 'co...</td>\n",
       "      <td>6.782524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13831</th>\n",
       "      <td>0</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.049287</td>\n",
       "      <td>0.244648</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.883645</td>\n",
       "      <td>['dec', '27', 'story', 'corrects', 'say', '550...</td>\n",
       "      <td>6.117021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13832</th>\n",
       "      <td>0</td>\n",
       "      <td>0.696581</td>\n",
       "      <td>-0.026939</td>\n",
       "      <td>0.293520</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.895276</td>\n",
       "      <td>['madrid', 'reuters', 'spain', 'high', 'court'...</td>\n",
       "      <td>6.478632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13833</th>\n",
       "      <td>1</td>\n",
       "      <td>0.719472</td>\n",
       "      <td>0.064587</td>\n",
       "      <td>0.499129</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.867533</td>\n",
       "      <td>['woman', 'fox', 'news', 'sick', 'tired', 'put...</td>\n",
       "      <td>6.102310</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13834 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label       TTR  text_polarity  text_subjectivity  title_polarity  \\\n",
       "0          1  0.844037       0.091481           0.524184       -0.125000   \n",
       "1          0  0.683438       0.003001           0.343395        0.000000   \n",
       "2          1  0.721030       0.073622           0.401345        0.000000   \n",
       "3          1  0.667638       0.037264           0.462935        0.000000   \n",
       "4          1  0.628032      -0.018966           0.479310       -0.800000   \n",
       "...      ...       ...            ...                ...             ...   \n",
       "13829      0  0.659631      -0.030698           0.380595        0.000000   \n",
       "13830      1  0.629126      -0.009035           0.312617       -0.066667   \n",
       "13831      0  0.595745       0.049287           0.244648        0.000000   \n",
       "13832      0  0.696581      -0.026939           0.293520        0.000000   \n",
       "13833      1  0.719472       0.064587           0.499129        0.000000   \n",
       "\n",
       "       title_subjectivity  doc_perplexity  \\\n",
       "0                0.125000        0.867938   \n",
       "1                0.000000        0.882860   \n",
       "2                1.000000        0.883760   \n",
       "3                1.000000        0.882882   \n",
       "4                0.900000        0.883120   \n",
       "...                   ...             ...   \n",
       "13829            0.000000        0.882810   \n",
       "13830            0.633333        0.895444   \n",
       "13831            0.000000        0.883645   \n",
       "13832            0.000000        0.895276   \n",
       "13833            0.000000        0.867533   \n",
       "\n",
       "                                                 1_grams  average_word_length  \n",
       "0      ['donald', 'trump', 'met', 'member', 'nato', '...             6.490826  \n",
       "1      ['washington', 'reuters', 'rick', 'perry', 'pr...             6.385744  \n",
       "2      ['president', 'obama', 'blasted', 'republican'...             6.072961  \n",
       "3      ['male', 'idaho', 'republican', 'daughter', 'c...             6.469388  \n",
       "4      ['kellyanne', 'conway', 'tried', 'spin', 'whit...             6.296496  \n",
       "...                                                  ...                  ...  \n",
       "13829  ['washington', 'reuters', 'member', 'congress'...             6.514512  \n",
       "13830  ['far', 'video', '530000', 'view', 'make', 'co...             6.782524  \n",
       "13831  ['dec', '27', 'story', 'corrects', 'say', '550...             6.117021  \n",
       "13832  ['madrid', 'reuters', 'spain', 'high', 'court'...             6.478632  \n",
       "13833  ['woman', 'fox', 'news', 'sick', 'tired', 'put...             6.102310  \n",
       "\n",
       "[13834 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('features_dataset.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label                    int64\n",
      "TTR                    float64\n",
      "text_polarity          float64\n",
      "text_subjectivity      float64\n",
      "title_polarity         float64\n",
      "title_subjectivity     float64\n",
      "doc_perplexity         float64\n",
      "1_grams                 object\n",
      "average_word_length    float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Print data types for each column\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean and seperate df to test and train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first we convert 1_grams to a numerical variable instead of object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Vectorize text data\n",
    "X_vectorized = vectorizer.fit_transform(df['1_grams'])\n",
    "\n",
    "# Drop 1_grams column and convert df to a sparse DataFrame to concatenate with X_vectorized\n",
    "df_dropped = df.drop('1_grams', axis=1).astype(float) # Convert other columns to float\n",
    "from scipy.sparse import csr_matrix, hstack\n",
    "df_dropped_sparse = csr_matrix(df_dropped.values)  # Convert DataFrame to sparse matrix\n",
    "X_final = hstack([df_dropped_sparse, X_vectorized])  # Concatenate the matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with negative values and their counts:\n",
      "text_polarity: 3097 negatives\n",
      "title_polarity: 3117 negatives\n"
     ]
    }
   ],
   "source": [
    "# convert columns to numeric and check for negative values\n",
    "def check_negatives(dataframe):\n",
    "    negative_counts = {}\n",
    "    for column in dataframe.columns:\n",
    "        # Convert column to numeric, non-convertible values become NaN\n",
    "        converted_column = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "        # Count negative values\n",
    "        count = (converted_column < 0).sum()\n",
    "        if count > 0:\n",
    "            negative_counts[column] = count\n",
    "    return negative_counts\n",
    "\n",
    "# Apply the function to your DataFrame\n",
    "negative_columns = check_negatives(df)\n",
    "\n",
    "# Print the columns with negative values and their counts\n",
    "print(\"Columns with negative values and their counts:\")\n",
    "for column, count in negative_columns.items():\n",
    "    print(f\"{column}: {count} negatives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_negatives_exclude_non_numeric(dataframe):\n",
    "    negative_counts = {}\n",
    "    for column in dataframe.columns:\n",
    "        # Attempt to convert column to numeric\n",
    "        converted_column = pd.to_numeric(dataframe[column], errors='coerce')\n",
    "        # Continue only if the column is numeric\n",
    "        if converted_column.dtype != 'object':\n",
    "            # Count negative values\n",
    "            count = (converted_column < 0).sum()\n",
    "            if count > 0:\n",
    "                negative_counts[column] = count\n",
    "    return negative_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all columns to numeric, handling non-numeric gracefully\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Fill NaN values that result from conversion errors\n",
    "df.fillna(0, inplace=True)\n",
    "\n",
    "# Initialize scaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data to scale each feature to [0, 1] range\n",
    "df_scaled = scaler.fit_transform(df.drop('label', axis=1))\n",
    "\n",
    "# Split the data into features and target\n",
    "X = df_scaled\n",
    "y = df['label']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and train model\n",
    "logistic_regression_model = LogisticRegression()\n",
    "logistic_regression_model.fit(X_train, y_train)\n",
    "\n",
    "# make predictions\n",
    "y_pred = logistic_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70      1353\n",
      "           1       0.72      0.68      0.70      1414\n",
      "\n",
      "    accuracy                           0.70      2767\n",
      "   macro avg       0.70      0.70      0.70      2767\n",
      "weighted avg       0.70      0.70      0.70      2767\n",
      "\n",
      "Confusion Matrix:\n",
      " [[975 378]\n",
      " [451 963]]\n",
      "Accuracy Score: 0.7003975424647633\n"
     ]
    }
   ],
   "source": [
    "# performance metrics\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning of Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l1; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.0s\n",
      "[CV] END ................................C=0.001, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END .................................C=0.01, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.1s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=0.1, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.2s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.2s\n",
      "[CV] END ....................................C=1, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.3s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=0.1, penalty=l1; total time=   0.2s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.3s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.3s\n",
      "[CV] END ...................................C=10, penalty=l2; total time=   0.0s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.2s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n",
      "[CV] END ....................................C=1, penalty=l1; total time=   0.3s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.0s\n",
      "[CV] END ..................................C=100, penalty=l2; total time=   0.0s\n",
      "[CV] END ...................................C=10, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.2s\n",
      "[CV] END ..................................C=100, penalty=l1; total time=   0.2s\n",
      "Best Parameters: {'C': 10, 'penalty': 'l2'}\n",
      "Best Score: 0.7081408985501212\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a logistic regression model with 'liblinear' solver\n",
    "logistic_regression = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Create a dictionary of all values you want to test for C and penalty\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],  # Regularization parameter\n",
    "    'penalty': ['l1', 'l2'],  # The norm used in the penalization\n",
    "}\n",
    "\n",
    "# Use GridSearchCV to search for the best parameters\n",
    "grid_search = GridSearchCV(logistic_regression, param_grid, cv=5, verbose=2, n_jobs=-1)\n",
    "\n",
    "# Fit the grid search model with your training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Output the best parameters and the best score\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Score:\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7003975424647633\n",
      "Confusion Matrix:\n",
      " [[975 378]\n",
      " [451 963]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.72      0.70      1353\n",
      "           1       0.72      0.68      0.70      1414\n",
      "\n",
      "    accuracy                           0.70      2767\n",
      "   macro avg       0.70      0.70      0.70      2767\n",
      "weighted avg       0.70      0.70      0.70      2767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Use the best estimator to make predictions on the test set\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "conf_mat = confusion_matrix(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Output the evaluation results\n",
    "print('Accuracy:', accuracy)\n",
    "print('Confusion Matrix:\\n', conf_mat)\n",
    "print('Classification Report:\\n', report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
