{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Understanding and Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "ngram_df = pd.read_csv('Preprocessed.csv')\n",
    "# List to store the n-gram feature DataFrames for each text column\n",
    "ngram_dfs = []\n",
    "\n",
    "# Iterate over each text column\n",
    "for column in ngram_df.select_dtypes(include=['object']).columns:\n",
    "    # Fill missing values with an empty string\n",
    "    corpus = ngram_df[column].fillna('').tolist()\n",
    "\n",
    "    # Initialize CountVectorizer for n-grams\n",
    "    vectorizer = CountVectorizer(ngram_range=(1, 3))  # Adjust n-gram range as needed\n",
    "\n",
    "    # Fit and transform the corpus\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    # Convert the result into a DataFrame\n",
    "    ngram_features_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Concatenate the n-gram features DataFrame with the original DataFrame\n",
    "   # ngram_df_with_features = pd.concat([ngram_df, ngram_features_df], axis=1)\n",
    "\n",
    "    # Append the DataFrame with n-gram features to the list\n",
    "    #ngram_dfs.append(ngram_df_with_features)\n",
    "\n",
    "# Concatenate all DataFrames in the list along the column axis\n",
    "#ngram_df_final = pd.concat(ngram_dfs, axis=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we perform a sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df = pd.read_csv('balanced_subset.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   label                                       cleaned_text  cleaned_subject  \\\n",
      "0      1  antonin scalia one reliable conservative u sup...  government news   \n",
      "1      0  reuters kansa vowed wednesday sue obama admini...     politicsnews   \n",
      "2      1  rtformer congressman ron paul revealed list fa...           usnews   \n",
      "3      1  thing looking good donald trump hillary clinto...             news   \n",
      "4      1  donald trump twitter account littered insult i...             news   \n",
      "\n",
      "                                       cleaned_title  token_count  \\\n",
      "0  breaking u supreme court justice antonin scali...           72   \n",
      "1  kansa latest u state challenge obama transgend...           71   \n",
      "2  ron paul highlight real list mainstream fake n...           73   \n",
      "3  watch hillary ruin trump right outside bankrup...           71   \n",
      "4  report aide take away donald trump twitter acc...           89   \n",
      "\n",
      "   text_polarity  text_subjectivity  title_polarity  title_subjectivity  \n",
      "0       0.169859           0.457757       -0.200000            0.400000  \n",
      "1       0.030584           0.298044        0.500000            0.900000  \n",
      "2      -0.020289           0.466312       -0.150000            0.650000  \n",
      "3       0.134733           0.501542        0.142857            0.292857  \n",
      "4      -0.014807           0.445003        0.000000            0.000000  \n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "# Function to apply sentiment analysis\n",
    "def analyze_sentiment(text):\n",
    "    testimonial = TextBlob(text)\n",
    "    return testimonial.sentiment.polarity, testimonial.sentiment.subjectivity\n",
    "\n",
    "# Applying sentiment analysis to the 'cleaned_text' and 'cleaned_title' columns\n",
    "sentiment_df['text_polarity'], sentiment_df['text_subjectivity'] = zip(*sentiment_df['cleaned_text'].apply(analyze_sentiment))\n",
    "sentiment_df['title_polarity'], sentiment_df['title_subjectivity'] = zip(*sentiment_df['cleaned_title'].apply(analyze_sentiment))\n",
    "\n",
    "# Now 'df' has four new columns with sentiment scores for both cleaned_text and cleaned_title\n",
    "print(sentiment_df.head())\n",
    "\n",
    "# Optionally, save the updated DataFrame with sentiment scores\n",
    "sentiment_df.to_csv('Preprocessed_incl_sentiment.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e8995cc79525bd2f03bf06fe7351d86fcc477a350b98b661267de2f13aa6250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
