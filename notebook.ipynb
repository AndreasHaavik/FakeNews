{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Understanding and Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21417"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake)\n",
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "21412  'Fully committed' NATO backs new U.S. approach...   \n",
       "21413  LexisNexis withdrew two products from Chinese ...   \n",
       "21414  Minsk cultural hub becomes haven from authorities   \n",
       "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
       "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date  label  \n",
       "0      December 31, 2017      1  \n",
       "1      December 31, 2017      1  \n",
       "2      December 30, 2017      1  \n",
       "3      December 29, 2017      1  \n",
       "4      December 25, 2017      1  \n",
       "...                  ...    ...  \n",
       "21412   August 22, 2017       0  \n",
       "21413   August 22, 2017       0  \n",
       "21414   August 22, 2017       0  \n",
       "21415   August 22, 2017       0  \n",
       "21416   August 22, 2017       0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake['label'] = 1\n",
    "true['label']=0\n",
    "\n",
    "df_concated = pd.concat([fake, true])\n",
    "permutation = np.random.permutation(len(df_concated))\n",
    "df = df_concated.iloc[permutation]\n",
    "df_concated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44689, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count_per_column = df.isnull().sum()\n",
    "\n",
    "print(nan_count_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18157</th>\n",
       "      <td>BREAKING NEWS: PRESIDENT TRUMP SPEAKS Out On #...</td>\n",
       "      <td>To his credit, President Donald Trump almost i...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21297</th>\n",
       "      <td>U.N. panel urges Russia to fight racism by neo...</td>\n",
       "      <td>GENEVA (Reuters) - A United Nations human righ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>MSNBC Producer Reminds Idiot Trump That ‘Morn...</td>\n",
       "      <td>Donald Trump spent his week on Twitter blastin...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10049</th>\n",
       "      <td>Obama urges Congress to take action on corpora...</td>\n",
       "      <td>WASHINGTON (Reuters) - U.S. President Barack O...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17355</th>\n",
       "      <td>A MUST SEE! THE BALTIMORE RIOT STORY IN ONE BI...</td>\n",
       "      <td></td>\n",
       "      <td>Government News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>Exclusive: Trump says Clinton policy on Syria ...</td>\n",
       "      <td>DORAL, Fla. (Reuters) - U.S. Republican presi...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>Trump Stooge: Women Making Assault Claims Are...</td>\n",
       "      <td>One of Donald Trump s top campaign officials h...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13154</th>\n",
       "      <td>OBAMA STONEWALLING ON Why He Sent 13 Payments ...</td>\n",
       "      <td>Leave it to the one and only Andrew McCarthy t...</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7626</th>\n",
       "      <td>Republican Caitlyn Jenner Makes OUTRAGEOUS Cl...</td>\n",
       "      <td>It s been really hard for the LGBT community t...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10832</th>\n",
       "      <td>Obama statement on death of Supreme Court Just...</td>\n",
       "      <td>RANCHO MIRAGE, Calif. (Reuters) - Following is...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "18157  BREAKING NEWS: PRESIDENT TRUMP SPEAKS Out On #...   \n",
       "21297  U.N. panel urges Russia to fight racism by neo...   \n",
       "955     MSNBC Producer Reminds Idiot Trump That ‘Morn...   \n",
       "10049  Obama urges Congress to take action on corpora...   \n",
       "17355  A MUST SEE! THE BALTIMORE RIOT STORY IN ONE BI...   \n",
       "...                                                  ...   \n",
       "7700   Exclusive: Trump says Clinton policy on Syria ...   \n",
       "4132    Trump Stooge: Women Making Assault Claims Are...   \n",
       "13154  OBAMA STONEWALLING ON Why He Sent 13 Payments ...   \n",
       "7626    Republican Caitlyn Jenner Makes OUTRAGEOUS Cl...   \n",
       "10832  Obama statement on death of Supreme Court Just...   \n",
       "\n",
       "                                                    text          subject  \\\n",
       "18157  To his credit, President Donald Trump almost i...        left-news   \n",
       "21297  GENEVA (Reuters) - A United Nations human righ...        worldnews   \n",
       "955    Donald Trump spent his week on Twitter blastin...             News   \n",
       "10049  WASHINGTON (Reuters) - U.S. President Barack O...     politicsNews   \n",
       "17355                                                     Government News   \n",
       "...                                                  ...              ...   \n",
       "7700    DORAL, Fla. (Reuters) - U.S. Republican presi...     politicsNews   \n",
       "4132   One of Donald Trump s top campaign officials h...             News   \n",
       "13154  Leave it to the one and only Andrew McCarthy t...         politics   \n",
       "7626   It s been really hard for the LGBT community t...             News   \n",
       "10832  RANCHO MIRAGE, Calif. (Reuters) - Following is...     politicsNews   \n",
       "\n",
       "       label  \n",
       "18157      1  \n",
       "21297      0  \n",
       "955        1  \n",
       "10049      0  \n",
       "17355      1  \n",
       "...      ...  \n",
       "7700       0  \n",
       "4132       1  \n",
       "13154      1  \n",
       "7626       1  \n",
       "10832      0  \n",
       "\n",
       "[44689 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_date = df.drop(columns=['date'])\n",
    "\n",
    "df_no_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to make text Preprocessing, where are going through these steps: \n",
    "* Lowercasing\n",
    "* Removing Special Characters and Punctuation\n",
    "* Tokenization\n",
    "* Removing Stopwords\n",
    "* Lemmatization (We'll prefer this over stemming as it's generally more effective for understanding the context of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')  # for stopwords\n",
    "nltk.download('wordnet')  # for lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_subject(subject):\n",
    "    # Convert text to lowercase\n",
    "    subject = subject.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    subject = ''.join(char for char in subject if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(subject)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_title(title):\n",
    "    # Convert text to lowercase\n",
    "    title = title.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    title = ''.join(char for char in title if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(title)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it contains a column named 'text'\n",
    "df_no_date['cleaned_text'] = df_no_date['text'].apply(preprocess_text)\n",
    "df_no_date['cleaned_subject'] = df_no_date['subject'].apply(preprocess_subject)\n",
    "df_no_date['cleaned_title'] = df_no_date['title'].apply(preprocess_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_date.drop(columns=['title', 'subject', 'text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_subject</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18157</th>\n",
       "      <td>1</td>\n",
       "      <td>credit president donald trump almost immediate...</td>\n",
       "      <td>leftnews</td>\n",
       "      <td>breaking news president trump speaks charlotte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21297</th>\n",
       "      <td>0</td>\n",
       "      <td>geneva reuters united nation human right panel...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>un panel urge russia fight racism neonazis sport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>955</th>\n",
       "      <td>1</td>\n",
       "      <td>donald trump spent week twitter blasting mika ...</td>\n",
       "      <td>news</td>\n",
       "      <td>msnbc producer reminds idiot trump morning joe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10049</th>\n",
       "      <td>0</td>\n",
       "      <td>washington reuters u president barack obama tu...</td>\n",
       "      <td>politicsnews</td>\n",
       "      <td>obama urge congress take action corporate tax ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17355</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>government news</td>\n",
       "      <td>must see baltimore riot story one big picture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7700</th>\n",
       "      <td>0</td>\n",
       "      <td>doral fla reuters u republican presidential no...</td>\n",
       "      <td>politicsnews</td>\n",
       "      <td>exclusive trump say clinton policy syria would...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4132</th>\n",
       "      <td>1</td>\n",
       "      <td>one donald trump top campaign official made gr...</td>\n",
       "      <td>news</td>\n",
       "      <td>trump stooge woman making assault claim ugly b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13154</th>\n",
       "      <td>1</td>\n",
       "      <td>leave one andrew mccarthy put together fantast...</td>\n",
       "      <td>politics</td>\n",
       "      <td>obama stonewalling sent 13 payment 9999999999 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7626</th>\n",
       "      <td>1</td>\n",
       "      <td>really hard lgbt community welcome caitlyn jen...</td>\n",
       "      <td>news</td>\n",
       "      <td>republican caitlyn jenner make outrageous clai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10832</th>\n",
       "      <td>0</td>\n",
       "      <td>rancho mirage calif reuters following full tex...</td>\n",
       "      <td>politicsnews</td>\n",
       "      <td>obama statement death supreme court justice sc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                       cleaned_text  \\\n",
       "18157      1  credit president donald trump almost immediate...   \n",
       "21297      0  geneva reuters united nation human right panel...   \n",
       "955        1  donald trump spent week twitter blasting mika ...   \n",
       "10049      0  washington reuters u president barack obama tu...   \n",
       "17355      1                                                      \n",
       "...      ...                                                ...   \n",
       "7700       0  doral fla reuters u republican presidential no...   \n",
       "4132       1  one donald trump top campaign official made gr...   \n",
       "13154      1  leave one andrew mccarthy put together fantast...   \n",
       "7626       1  really hard lgbt community welcome caitlyn jen...   \n",
       "10832      0  rancho mirage calif reuters following full tex...   \n",
       "\n",
       "       cleaned_subject                                      cleaned_title  \n",
       "18157         leftnews  breaking news president trump speaks charlotte...  \n",
       "21297        worldnews   un panel urge russia fight racism neonazis sport  \n",
       "955               news  msnbc producer reminds idiot trump morning joe...  \n",
       "10049     politicsnews  obama urge congress take action corporate tax ...  \n",
       "17355  government news      must see baltimore riot story one big picture  \n",
       "...                ...                                                ...  \n",
       "7700      politicsnews  exclusive trump say clinton policy syria would...  \n",
       "4132              news  trump stooge woman making assault claim ugly b...  \n",
       "13154         politics  obama stonewalling sent 13 payment 9999999999 ...  \n",
       "7626              news  republican caitlyn jenner make outrageous clai...  \n",
       "10832     politicsnews  obama statement death supreme court justice sc...  \n",
       "\n",
       "[44689 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_date.to_csv('Preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "label                0\n",
      "cleaned_text       632\n",
      "cleaned_subject      0\n",
      "cleaned_title        0\n",
      "dtype: int64\n",
      "Total missing values in the dataset: 632\n"
     ]
    }
   ],
   "source": [
    "# Load your DataFrame\n",
    "df = pd.read_csv('Preprocessed.csv')\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values in each column\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# Calculate the total number of missing values in the DataFrame\n",
    "total_missing_values = missing_values_count.sum()\n",
    "print(\"Total missing values in the dataset:\", total_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our models and other features are able to be included and understood, we remove the rows with missing values below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (44689, 4)\n",
      "New DataFrame shape after removing rows with NaN: (44057, 4)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values in cleaned_text as seen from the printout above\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new file if needed\n",
    "df_cleaned.to_csv('Preprocessed_clean.csv', index=False)\n",
    "\n",
    "# Optionally, print the shape to see how many rows were dropped\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"New DataFrame shape after removing rows with NaN:\", df_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we vectorize the dataset to enable counting tokens in the texgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                       cleaned_text  \\\n",
      "0          1  credit president donald trump almost immediate...   \n",
      "1          0  geneva reuters united nation human right panel...   \n",
      "2          1  donald trump spent week twitter blasting mika ...   \n",
      "3          0  washington reuters u president barack obama tu...   \n",
      "4          0  athens reuters two gunman fired round shot hea...   \n",
      "...      ...                                                ...   \n",
      "44052      0  doral fla reuters u republican presidential no...   \n",
      "44053      1  one donald trump top campaign official made gr...   \n",
      "44054      1  leave one andrew mccarthy put together fantast...   \n",
      "44055      1  really hard lgbt community welcome caitlyn jen...   \n",
      "44056      0  rancho mirage calif reuters following full tex...   \n",
      "\n",
      "      cleaned_subject                                      cleaned_title  \\\n",
      "0            leftnews  breaking news president trump speaks charlotte...   \n",
      "1           worldnews   un panel urge russia fight racism neonazis sport   \n",
      "2                news  msnbc producer reminds idiot trump morning joe...   \n",
      "3        politicsnews  obama urge congress take action corporate tax ...   \n",
      "4           worldnews  gunman fire shot greece socialist party headqu...   \n",
      "...               ...                                                ...   \n",
      "44052    politicsnews  exclusive trump say clinton policy syria would...   \n",
      "44053            news  trump stooge woman making assault claim ugly b...   \n",
      "44054        politics  obama stonewalling sent 13 payment 9999999999 ...   \n",
      "44055            news  republican caitlyn jenner make outrageous clai...   \n",
      "44056    politicsnews  obama statement death supreme court justice sc...   \n",
      "\n",
      "       token_count  \n",
      "0               49  \n",
      "1               68  \n",
      "2               54  \n",
      "3               47  \n",
      "4               33  \n",
      "...            ...  \n",
      "44052           97  \n",
      "44053           59  \n",
      "44054           87  \n",
      "44055           70  \n",
      "44056           60  \n",
      "\n",
      "[44057 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset\n",
    "subset_df = pd.read_csv('Preprocessed_clean.csv')\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer with max of 512 to fit the best practice of BERT while also decreasing the size of the dataset\n",
    "vectorizer = TfidfVectorizer(max_features=512)  # You can adjust this if needed\n",
    "\n",
    "# Apply TF-IDF to the 'cleaned_text' column\n",
    "tfidf_matrix = vectorizer.fit_transform(subset_df['cleaned_text'])\n",
    "\n",
    "# Convert TF-IDF matrix to a DataFrame to manipulate easily\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Count non-zero entries in each row, which represents the number of unique tokens used\n",
    "subset_df['token_count'] = (tfidf_df != 0).sum(axis=1)\n",
    "\n",
    "# Filter out rows where the number of tokens is greater than 512\n",
    "filtered_df = subset_df[subset_df['token_count'] <= 512]\n",
    "\n",
    "# Saving the filtered DataFrame\n",
    "filtered_df.to_csv('filtered_dataset.csv', index=False)\n",
    "\n",
    "# We then print the dataset to inspect whether this code removed any of the rows. \n",
    "print(filtered_df.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        49\n",
      "1        68\n",
      "2        54\n",
      "3        47\n",
      "4        33\n",
      "         ..\n",
      "44052    97\n",
      "44053    59\n",
      "44054    87\n",
      "44055    70\n",
      "44056    60\n",
      "Name: token_count, Length: 44057, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df['token_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the printed results none of the rows have been removed, as none of them are above 512\n",
    "Below we examine the token_count, to determine wether we are able to use this as a way to create subsets and decrease the size of the dataset\n",
    "We will examine it by looking at both the lowest and highest token_count to ensure we incorporate relevant articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                       cleaned_text  \\\n",
      "29518      1  funny secret travel start believe bleed lyric ...   \n",
      "36783      1  funny secret travel start believe bleed lyric ...   \n",
      "\n",
      "      cleaned_subject                                      cleaned_title  \\\n",
      "29518      middleeast  medium tripwire ping pong pizza conspiracy pro...   \n",
      "36783          usnews  medium tripwire ping pong pizza conspiracy pro...   \n",
      "\n",
      "       token_count  \n",
      "29518          360  \n",
      "36783          360  \n",
      "       label                                      cleaned_text  \\\n",
      "9          1             charlie leduff legend detroit classic   \n",
      "448        1  httpswwwyoutubecomwatchtimecontinue2vijwclqckhd8   \n",
      "731        1                httpswwwyoutubecomwatchvptbfkqk7gu   \n",
      "1128       1                                           wow wow   \n",
      "1299       1                                             enjoy   \n",
      "...      ...                                               ...   \n",
      "42080      1                                         brave guy   \n",
      "42858      1                                    wow dems touch   \n",
      "42887      1               httpswwwyoutubecomwatchvyrxmfmgoptk   \n",
      "43646      1               httpswwwyoutubecomwatchvj4ljxrofef8   \n",
      "43682      1                           httpsyoutube7oohwhg2gb4   \n",
      "\n",
      "       cleaned_subject                                      cleaned_title  \\\n",
      "9      government news  comedy gold detroit news willy dump tire wrong...   \n",
      "448    government news  trump cabinet member mick mulvaney dc place mu...   \n",
      "731           leftnews  liberal bigot destroyed legendary democrat ala...   \n",
      "1128          politics  rush limbaugh warns gop choose trump cruz dont...   \n",
      "1299          politics  ouch paul joseph watson destroys mtvs racist p...   \n",
      "...                ...                                                ...   \n",
      "42080         politics  reporter asks obama golf instead attend scalia...   \n",
      "42858         leftnews  kidding here hillary supporter get u killed video   \n",
      "42887  government news  judge jeanine pirros truth bomb fired u attorn...   \n",
      "43646         politics  wow tucker jesse destroy liberal kook protesti...   \n",
      "43682         politics  tucker carlson destroys smug elector refuse vo...   \n",
      "\n",
      "       token_count  \n",
      "9                0  \n",
      "448              0  \n",
      "731              0  \n",
      "1128             0  \n",
      "1299             0  \n",
      "...            ...  \n",
      "42080            0  \n",
      "42858            0  \n",
      "42887            0  \n",
      "43646            0  \n",
      "43682            0  \n",
      "\n",
      "[134 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the maximum 'token_count'\n",
    "max_token_row = filtered_df[filtered_df['token_count'] == filtered_df['token_count'].max()]\n",
    "\n",
    "# Find the row with the minimum 'token_count'\n",
    "min_token_row = filtered_df[filtered_df['token_count'] == filtered_df['token_count'].min()]\n",
    "\n",
    "# Print the row with the maximum and minimum token count\n",
    "print(max_token_row)\n",
    "print(min_token_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see many rows seem to have 0 tokens, which means these are not understood well by the computer\n",
    "This show a relevance in setting a minimum token_count and creating a subset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: (13834, 5)\n",
      "Class distribution in subset:\n",
      " 1    6917\n",
      "0    6917\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a minimum token count threshold\n",
    "MIN_TOKENS = 70\n",
    "\n",
    "# determine that the DataFrame should include only rows with 'token_count' being greater than or equal to MIN_TOKENS\n",
    "filter_df_tokens = filtered_df[filtered_df['token_count'] >= MIN_TOKENS]\n",
    "\n",
    "# Split the DataFrame into two based on the label\n",
    "tokens_df_class_0 = filter_df_tokens[filter_df_tokens['label'] == 0]\n",
    "tokens_df_class_1 = filter_df_tokens[filter_df_tokens['label'] == 1]\n",
    "\n",
    "# Find the minimum count to balance the dataset\n",
    "min_count = min(len(tokens_df_class_0), len(tokens_df_class_1))\n",
    "\n",
    "# Randomly sample min_count rows from each DataFrame\n",
    "subset_class_0 = tokens_df_class_0.sample(n=min_count, random_state=42)  # Ensures reproducibility\n",
    "subset_class_1 = tokens_df_class_1.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Concatenate the two subsets to form a new balanced DataFrame\n",
    "balanced_subset = pd.concat([subset_class_0, subset_class_1])\n",
    "\n",
    "# Shuffle the rows to ensure random order\n",
    "final_subset = balanced_subset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the subset to a new CSV file\n",
    "final_subset.to_csv('balanced_subset.csv', index=False)\n",
    "\n",
    "# Print some information about the subset\n",
    "print(\"Subset shape:\", final_subset.shape)\n",
    "print(\"Class distribution in subset:\\n\", final_subset['label'].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e8995cc79525bd2f03bf06fe7351d86fcc477a350b98b661267de2f13aa6250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
