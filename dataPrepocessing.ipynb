{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Understanding and Preparation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake = pd.read_csv('Fake.csv')\n",
    "true = pd.read_csv('True.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21417"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fake)\n",
    "len(true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Donald Trump Sends Out Embarrassing New Year’...</td>\n",
       "      <td>Donald Trump just couldn t wish all Americans ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Drunk Bragging Trump Staffer Started Russian ...</td>\n",
       "      <td>House Intelligence Committee Chairman Devin Nu...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 31, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sheriff David Clarke Becomes An Internet Joke...</td>\n",
       "      <td>On Friday, it was revealed that former Milwauk...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 30, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Trump Is So Obsessed He Even Has Obama’s Name...</td>\n",
       "      <td>On Christmas day, Donald Trump announced that ...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 29, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Pope Francis Just Called Out Donald Trump Dur...</td>\n",
       "      <td>Pope Francis used his annual Christmas Day mes...</td>\n",
       "      <td>News</td>\n",
       "      <td>December 25, 2017</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21412</th>\n",
       "      <td>'Fully committed' NATO backs new U.S. approach...</td>\n",
       "      <td>BRUSSELS (Reuters) - NATO allies on Tuesday we...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21413</th>\n",
       "      <td>LexisNexis withdrew two products from Chinese ...</td>\n",
       "      <td>LONDON (Reuters) - LexisNexis, a provider of l...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21414</th>\n",
       "      <td>Minsk cultural hub becomes haven from authorities</td>\n",
       "      <td>MINSK (Reuters) - In the shadow of disused Sov...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21415</th>\n",
       "      <td>Vatican upbeat on possibility of Pope Francis ...</td>\n",
       "      <td>MOSCOW (Reuters) - Vatican Secretary of State ...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21416</th>\n",
       "      <td>Indonesia to buy $1.14 billion worth of Russia...</td>\n",
       "      <td>JAKARTA (Reuters) - Indonesia will buy 11 Sukh...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>August 22, 2017</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44898 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "0       Donald Trump Sends Out Embarrassing New Year’...   \n",
       "1       Drunk Bragging Trump Staffer Started Russian ...   \n",
       "2       Sheriff David Clarke Becomes An Internet Joke...   \n",
       "3       Trump Is So Obsessed He Even Has Obama’s Name...   \n",
       "4       Pope Francis Just Called Out Donald Trump Dur...   \n",
       "...                                                  ...   \n",
       "21412  'Fully committed' NATO backs new U.S. approach...   \n",
       "21413  LexisNexis withdrew two products from Chinese ...   \n",
       "21414  Minsk cultural hub becomes haven from authorities   \n",
       "21415  Vatican upbeat on possibility of Pope Francis ...   \n",
       "21416  Indonesia to buy $1.14 billion worth of Russia...   \n",
       "\n",
       "                                                    text    subject  \\\n",
       "0      Donald Trump just couldn t wish all Americans ...       News   \n",
       "1      House Intelligence Committee Chairman Devin Nu...       News   \n",
       "2      On Friday, it was revealed that former Milwauk...       News   \n",
       "3      On Christmas day, Donald Trump announced that ...       News   \n",
       "4      Pope Francis used his annual Christmas Day mes...       News   \n",
       "...                                                  ...        ...   \n",
       "21412  BRUSSELS (Reuters) - NATO allies on Tuesday we...  worldnews   \n",
       "21413  LONDON (Reuters) - LexisNexis, a provider of l...  worldnews   \n",
       "21414  MINSK (Reuters) - In the shadow of disused Sov...  worldnews   \n",
       "21415  MOSCOW (Reuters) - Vatican Secretary of State ...  worldnews   \n",
       "21416  JAKARTA (Reuters) - Indonesia will buy 11 Sukh...  worldnews   \n",
       "\n",
       "                    date  label  \n",
       "0      December 31, 2017      1  \n",
       "1      December 31, 2017      1  \n",
       "2      December 30, 2017      1  \n",
       "3      December 29, 2017      1  \n",
       "4      December 25, 2017      1  \n",
       "...                  ...    ...  \n",
       "21412   August 22, 2017       0  \n",
       "21413   August 22, 2017       0  \n",
       "21414   August 22, 2017       0  \n",
       "21415   August 22, 2017       0  \n",
       "21416   August 22, 2017       0  \n",
       "\n",
       "[44898 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake['label'] = 1\n",
    "true['label']=0\n",
    "\n",
    "df_concated = pd.concat([fake, true])\n",
    "permutation = np.random.permutation(len(df_concated))\n",
    "df = df_concated.iloc[permutation]\n",
    "df_concated\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "209"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(44689, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.drop_duplicates()\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title      0\n",
      "text       0\n",
      "subject    0\n",
      "date       0\n",
      "label      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_count_per_column = df.isnull().sum()\n",
    "\n",
    "print(nan_count_per_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6547</th>\n",
       "      <td>CNN Just Accidentally Aired Girl Flipping Off...</td>\n",
       "      <td>After Donald Trump won the Indiana primary on ...</td>\n",
       "      <td>News</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11052</th>\n",
       "      <td>200 SOCIALIST RADICALS Storm Heritage Foundati...</td>\n",
       "      <td>A group called  People s Action  just held the...</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>Syrian army nears Islamic State stronghold al-...</td>\n",
       "      <td>BEIRUT (Reuters) - Syria s army and its allies...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>SHOCKING PHYSICAL ABUSE REVEALED: Former Secre...</td>\n",
       "      <td>Trump was 100% correct when he said,  Hillary ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19610</th>\n",
       "      <td>BUH-BYE! GLENN BECK Places Final Nail In His C...</td>\n",
       "      <td>Glenn Beck has just proven once again, (to any...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11478</th>\n",
       "      <td>CHILLING! FOX REPORTER JAMES ROSEN Recounts Be...</td>\n",
       "      <td></td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19036</th>\n",
       "      <td>DOES NANCY NEED MEDICAL ATTENTION? Watch As Na...</td>\n",
       "      <td>Thank you to the American Mirror for putting t...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11028</th>\n",
       "      <td>COLLEGE STUDENTS EXPRESS DISGUST In Trump’s “F...</td>\n",
       "      <td>Campus Reform   Saturday will mark Donald Trum...</td>\n",
       "      <td>politics</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>U.S. lawmakers want more Iran sanctions, but c...</td>\n",
       "      <td>WASHINGTON (Reuters) - A senior U.S. senator s...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>Highlights: The Trump presidency on January 27...</td>\n",
       "      <td>(Reuters) - Highlights of the day for U.S. Pre...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "6547    CNN Just Accidentally Aired Girl Flipping Off...   \n",
       "11052  200 SOCIALIST RADICALS Storm Heritage Foundati...   \n",
       "18151  Syrian army nears Islamic State stronghold al-...   \n",
       "13615  SHOCKING PHYSICAL ABUSE REVEALED: Former Secre...   \n",
       "19610  BUH-BYE! GLENN BECK Places Final Nail In His C...   \n",
       "...                                                  ...   \n",
       "11478  CHILLING! FOX REPORTER JAMES ROSEN Recounts Be...   \n",
       "19036  DOES NANCY NEED MEDICAL ATTENTION? Watch As Na...   \n",
       "11028  COLLEGE STUDENTS EXPRESS DISGUST In Trump’s “F...   \n",
       "9253   U.S. lawmakers want more Iran sanctions, but c...   \n",
       "5950   Highlights: The Trump presidency on January 27...   \n",
       "\n",
       "                                                    text       subject  label  \n",
       "6547   After Donald Trump won the Indiana primary on ...          News      1  \n",
       "11052  A group called  People s Action  just held the...      politics      1  \n",
       "18151  BEIRUT (Reuters) - Syria s army and its allies...     worldnews      0  \n",
       "13615  Trump was 100% correct when he said,  Hillary ...      politics      1  \n",
       "19610  Glenn Beck has just proven once again, (to any...     left-news      1  \n",
       "...                                                  ...           ...    ...  \n",
       "11478                                                         politics      1  \n",
       "19036  Thank you to the American Mirror for putting t...     left-news      1  \n",
       "11028  Campus Reform   Saturday will mark Donald Trum...      politics      1  \n",
       "9253   WASHINGTON (Reuters) - A senior U.S. senator s...  politicsNews      0  \n",
       "5950   (Reuters) - Highlights of the day for U.S. Pre...  politicsNews      0  \n",
       "\n",
       "[44689 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_date = df.drop(columns=['date'])\n",
    "\n",
    "df_no_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets try to make text Preprocessing, where are going through these steps: \n",
    "* Lowercasing\n",
    "* Removing Special Characters and Punctuation\n",
    "* Tokenization\n",
    "* Removing Stopwords\n",
    "* Lemmatization (We'll prefer this over stemming as it's generally more effective for understanding the context of words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/claramillekalo/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')  # for tokenization\n",
    "nltk.download('stopwords')  # for stopwords\n",
    "nltk.download('wordnet')  # for lemmatization\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_subject(subject):\n",
    "    # Convert text to lowercase\n",
    "    subject = subject.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    subject = ''.join(char for char in subject if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(subject)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def preprocess_title(title):\n",
    "    # Convert text to lowercase\n",
    "    title = title.lower()\n",
    "\n",
    "    # Remove special characters and punctuation\n",
    "    title = ''.join(char for char in title if char.isalnum() or char.isspace())\n",
    "\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(title)\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "    # Join words back to form the cleaned text\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Assuming 'df' is your DataFrame and it contains a column named 'text'\n",
    "df_no_date['cleaned_text'] = df_no_date['text'].apply(preprocess_text)\n",
    "df_no_date['cleaned_subject'] = df_no_date['subject'].apply(preprocess_subject)\n",
    "df_no_date['cleaned_title'] = df_no_date['title'].apply(preprocess_title)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_date.drop(columns=['title', 'subject', 'text'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_subject</th>\n",
       "      <th>cleaned_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6547</th>\n",
       "      <td>1</td>\n",
       "      <td>donald trump indiana primary tuesday ted cruz ...</td>\n",
       "      <td>news</td>\n",
       "      <td>cnn accidentally aired girl flipping trump tow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11052</th>\n",
       "      <td>1</td>\n",
       "      <td>group called people action held big gathering ...</td>\n",
       "      <td>politics</td>\n",
       "      <td>200 socialist radical storm heritage foundatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18151</th>\n",
       "      <td>0</td>\n",
       "      <td>beirut reuters syria army ally neared almayadi...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>syrian army nears islamic state stronghold alm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>1</td>\n",
       "      <td>trump 100 correct said hillary temperament bec...</td>\n",
       "      <td>politics</td>\n",
       "      <td>shocking physical abuse revealed former secret...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19610</th>\n",
       "      <td>1</td>\n",
       "      <td>glenn beck proven anyone still give darn say l...</td>\n",
       "      <td>leftnews</td>\n",
       "      <td>buhbye glenn beck place final nail coffinand f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11478</th>\n",
       "      <td>1</td>\n",
       "      <td></td>\n",
       "      <td>politics</td>\n",
       "      <td>chilling fox reporter james rosen recount spie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19036</th>\n",
       "      <td>1</td>\n",
       "      <td>thank american mirror putting video together s...</td>\n",
       "      <td>leftnews</td>\n",
       "      <td>nancy need medical attention watch nancy pelos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11028</th>\n",
       "      <td>1</td>\n",
       "      <td>campus reform saturday mark donald trump 100th...</td>\n",
       "      <td>politics</td>\n",
       "      <td>college student express disgust trump first 10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9253</th>\n",
       "      <td>0</td>\n",
       "      <td>washington reuters senior u senator said tuesd...</td>\n",
       "      <td>politicsnews</td>\n",
       "      <td>u lawmaker want iran sanction cant agree</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5950</th>\n",
       "      <td>0</td>\n",
       "      <td>reuters highlight day u president donald trump...</td>\n",
       "      <td>politicsnews</td>\n",
       "      <td>highlight trump presidency january 27 243 pm e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>44689 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                       cleaned_text  \\\n",
       "6547       1  donald trump indiana primary tuesday ted cruz ...   \n",
       "11052      1  group called people action held big gathering ...   \n",
       "18151      0  beirut reuters syria army ally neared almayadi...   \n",
       "13615      1  trump 100 correct said hillary temperament bec...   \n",
       "19610      1  glenn beck proven anyone still give darn say l...   \n",
       "...      ...                                                ...   \n",
       "11478      1                                                      \n",
       "19036      1  thank american mirror putting video together s...   \n",
       "11028      1  campus reform saturday mark donald trump 100th...   \n",
       "9253       0  washington reuters senior u senator said tuesd...   \n",
       "5950       0  reuters highlight day u president donald trump...   \n",
       "\n",
       "      cleaned_subject                                      cleaned_title  \n",
       "6547             news  cnn accidentally aired girl flipping trump tow...  \n",
       "11052        politics  200 socialist radical storm heritage foundatio...  \n",
       "18151       worldnews  syrian army nears islamic state stronghold alm...  \n",
       "13615        politics  shocking physical abuse revealed former secret...  \n",
       "19610        leftnews  buhbye glenn beck place final nail coffinand f...  \n",
       "...               ...                                                ...  \n",
       "11478        politics  chilling fox reporter james rosen recount spie...  \n",
       "19036        leftnews  nancy need medical attention watch nancy pelos...  \n",
       "11028        politics  college student express disgust trump first 10...  \n",
       "9253     politicsnews           u lawmaker want iran sanction cant agree  \n",
       "5950     politicsnews  highlight trump presidency january 27 243 pm e...  \n",
       "\n",
       "[44689 rows x 4 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_date.to_csv('Preprocessed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      "label                0\n",
      "cleaned_text       632\n",
      "cleaned_subject      0\n",
      "cleaned_title        0\n",
      "dtype: int64\n",
      "Total missing values in the dataset: 632\n"
     ]
    }
   ],
   "source": [
    "# Load your DataFrame\n",
    "df = pd.read_csv('Preprocessed.csv')\n",
    "\n",
    "# Count the number of missing values in each column\n",
    "missing_values_count = df.isnull().sum()\n",
    "\n",
    "# Print the number of missing values in each column\n",
    "print(\"Missing values in each column:\")\n",
    "print(missing_values_count)\n",
    "\n",
    "# Calculate the total number of missing values in the DataFrame\n",
    "total_missing_values = missing_values_count.sum()\n",
    "print(\"Total missing values in the dataset:\", total_missing_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To ensure that our models and other features are able to be included and understood, we remove the rows with missing values below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame shape: (44689, 4)\n",
      "New DataFrame shape after removing rows with NaN: (44057, 4)\n"
     ]
    }
   ],
   "source": [
    "# Remove rows with missing values in cleaned_text as seen from the printout above\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Save the cleaned DataFrame to a new file if needed\n",
    "df_cleaned.to_csv('Preprocessed_clean.csv', index=False)\n",
    "\n",
    "# Optionally, print the shape to see how many rows were dropped\n",
    "print(\"Original DataFrame shape:\", df.shape)\n",
    "print(\"New DataFrame shape after removing rows with NaN:\", df_cleaned.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we vectorize the dataset to enable counting tokens in the texgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m filtered_df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfiltered_dataset.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# We then print the dataset to inspect whether this code removed any of the rows. \u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28mprint\u001b[39m(filtered_df\u001b[38;5;241m.\u001b[39mtokens)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__getattribute__\u001b[39m(\u001b[39mself\u001b[39m, name)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'tokens'"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Load your dataset\n",
    "subset_df = pd.read_csv('Preprocessed_clean.csv')\n",
    "\n",
    "# Initialize a TF-IDF Vectorizer with max of 512 to fit the best practice of BERT while also decreasing the size of the dataset\n",
    "vectorizer = TfidfVectorizer(max_features=512)  # You can adjust this if needed\n",
    "\n",
    "# Apply TF-IDF to the 'cleaned_text' column\n",
    "tfidf_matrix = vectorizer.fit_transform(subset_df['cleaned_text'])\n",
    "\n",
    "# Convert TF-IDF matrix to a DataFrame to manipulate easily\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Count non-zero entries in each row, which represents the number of unique tokens used\n",
    "subset_df['token_count'] = (tfidf_df != 0).sum(axis=1)\n",
    "\n",
    "# Filter out rows where the number of tokens is greater than 512\n",
    "filtered_df = subset_df[subset_df['token_count'] <= 512]\n",
    "\n",
    "# Saving the filtered DataFrame\n",
    "filtered_df.to_csv('filtered_dataset.csv', index=False)\n",
    "\n",
    "# We then print the dataset to inspect whether this code removed any of the rows. \n",
    "print(filtered_df.tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        49\n",
      "1        68\n",
      "2        54\n",
      "3        47\n",
      "4        33\n",
      "         ..\n",
      "44052    97\n",
      "44053    59\n",
      "44054    87\n",
      "44055    70\n",
      "44056    60\n",
      "Name: token_count, Length: 44057, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(filtered_df['token_count'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen from the printed results none of the rows have been removed, as none of them are above 512\n",
    "Below we examine the token_count, to determine wether we are able to use this as a way to create subsets and decrease the size of the dataset\n",
    "We will examine it by looking at both the lowest and highest token_count to ensure we incorporate relevant articles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       label                                       cleaned_text  \\\n",
      "29518      1  funny secret travel start believe bleed lyric ...   \n",
      "36783      1  funny secret travel start believe bleed lyric ...   \n",
      "\n",
      "      cleaned_subject                                      cleaned_title  \\\n",
      "29518      middleeast  medium tripwire ping pong pizza conspiracy pro...   \n",
      "36783          usnews  medium tripwire ping pong pizza conspiracy pro...   \n",
      "\n",
      "       token_count  \n",
      "29518          360  \n",
      "36783          360  \n",
      "       label                                      cleaned_text  \\\n",
      "9          1             charlie leduff legend detroit classic   \n",
      "448        1  httpswwwyoutubecomwatchtimecontinue2vijwclqckhd8   \n",
      "731        1                httpswwwyoutubecomwatchvptbfkqk7gu   \n",
      "1128       1                                           wow wow   \n",
      "1299       1                                             enjoy   \n",
      "...      ...                                               ...   \n",
      "42080      1                                         brave guy   \n",
      "42858      1                                    wow dems touch   \n",
      "42887      1               httpswwwyoutubecomwatchvyrxmfmgoptk   \n",
      "43646      1               httpswwwyoutubecomwatchvj4ljxrofef8   \n",
      "43682      1                           httpsyoutube7oohwhg2gb4   \n",
      "\n",
      "       cleaned_subject                                      cleaned_title  \\\n",
      "9      government news  comedy gold detroit news willy dump tire wrong...   \n",
      "448    government news  trump cabinet member mick mulvaney dc place mu...   \n",
      "731           leftnews  liberal bigot destroyed legendary democrat ala...   \n",
      "1128          politics  rush limbaugh warns gop choose trump cruz dont...   \n",
      "1299          politics  ouch paul joseph watson destroys mtvs racist p...   \n",
      "...                ...                                                ...   \n",
      "42080         politics  reporter asks obama golf instead attend scalia...   \n",
      "42858         leftnews  kidding here hillary supporter get u killed video   \n",
      "42887  government news  judge jeanine pirros truth bomb fired u attorn...   \n",
      "43646         politics  wow tucker jesse destroy liberal kook protesti...   \n",
      "43682         politics  tucker carlson destroys smug elector refuse vo...   \n",
      "\n",
      "       token_count  \n",
      "9                0  \n",
      "448              0  \n",
      "731              0  \n",
      "1128             0  \n",
      "1299             0  \n",
      "...            ...  \n",
      "42080            0  \n",
      "42858            0  \n",
      "42887            0  \n",
      "43646            0  \n",
      "43682            0  \n",
      "\n",
      "[134 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "# Find the row with the maximum 'token_count'\n",
    "max_token_row = filtered_df[filtered_df['token_count'] == filtered_df['token_count'].max()]\n",
    "\n",
    "# Find the row with the minimum 'token_count'\n",
    "min_token_row = filtered_df[filtered_df['token_count'] == filtered_df['token_count'].min()]\n",
    "\n",
    "# Print the row with the maximum and minimum token count\n",
    "print(max_token_row)\n",
    "print(min_token_row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can clearly see many rows seem to have 0 tokens, which means these are not understood well by the computer\n",
    "This show a relevance in setting a minimum token_count and creating a subset based on this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subset shape: (13834, 5)\n",
      "Class distribution in subset:\n",
      " 1    6917\n",
      "0    6917\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Define a minimum token count threshold\n",
    "MIN_TOKENS = 70\n",
    "\n",
    "# determine that the DataFrame should include only rows with 'token_count' being greater than or equal to MIN_TOKENS\n",
    "filter_df_tokens = filtered_df[filtered_df['token_count'] >= MIN_TOKENS]\n",
    "\n",
    "# Split the DataFrame into two based on the label\n",
    "tokens_df_class_0 = filter_df_tokens[filter_df_tokens['label'] == 0]\n",
    "tokens_df_class_1 = filter_df_tokens[filter_df_tokens['label'] == 1]\n",
    "\n",
    "# Find the minimum count to balance the dataset\n",
    "min_count = min(len(tokens_df_class_0), len(tokens_df_class_1))\n",
    "\n",
    "# Randomly sample min_count rows from each DataFrame\n",
    "subset_class_0 = tokens_df_class_0.sample(n=min_count, random_state=42)  # Ensures reproducibility\n",
    "subset_class_1 = tokens_df_class_1.sample(n=min_count, random_state=42)\n",
    "\n",
    "# Concatenate the two subsets to form a new balanced DataFrame\n",
    "balanced_subset = pd.concat([subset_class_0, subset_class_1])\n",
    "\n",
    "# Shuffle the rows to ensure random order\n",
    "final_subset = balanced_subset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "# Optionally, save the subset to a new CSV file\n",
    "final_subset.to_csv('balanced_subset.csv', index=False)\n",
    "\n",
    "# Print some information about the subset\n",
    "print(\"Subset shape:\", final_subset.shape)\n",
    "print(\"Class distribution in subset:\\n\", final_subset['label'].value_counts())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "9e8995cc79525bd2f03bf06fe7351d86fcc477a350b98b661267de2f13aa6250"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
